DBSCAN (density based spatial clustering of applications with noise). Worst case runtime of O(n**2)

Its a clustering algorithm that makes no assumptions about the number or shape of cluster (unlike k-means).
1) A random observation Xi is selected.
2) If it has a min of close neighbours j, its part of a cluster.
3) step 2) is repeated recursively for all of Xi's neighbors, their neighbors, etc These are the cluster's core members
4) Once step 3 runs out of observations, a new random point is chosen.

Observations not part of a core are assigned to a nearby cluster or marked as outliers.

Pros-
1) Robust to outliers due to in-built concept of noise
2) Designed for Databases that can accelerate region queries using R* trees and make use of indexes
3) Params minPts and e for circle radius can be set by a domain expert, if data is well understood.

Cons -
1) Not entirely deterministic (but its determinsitic on core points and noise points),
    border points can belong to either cluster. depends on the order data is processed in. This situation rarely occurs
    in practice though.

2) Distance metrics used, if euclidean distance, can be useless for high-dimensional sparse data, just like for any
    other clustering algo using this distance metric.

3) Cannot cluster well when there are large differences in densities between clusters, as minPts and ε cannot be chosen
    accurately.


--------------------
t-SNE - t-Distributed Stochastic neighbour embedding is an algo for visualizing embeddings in 2-d space after
dimensionality reduction from multi-dimensional space such that clusters form for nearest neighbors in multi-D space.

--------------------
SUPPORT VECTOR MACHINES (SVM) CONCEPTS AND IMPORTANT PROPERTIES -
1) Try to find the maximal hyperplane differentiating the 2 classes in binary classification.

2) The margins of the hyperplane are chosen such that the data points lying on the margins are called the support
    vectors and should belong to opposite classes on either margin.

3) The distance between each margin and the MMH ie the parallel line midway between the 2 margins is given by
    the formula 1 / || W ||
    ie, the norm of W = sqrt(w1^2 + w2^2 + ... wn^2) where n = number of support vectors. So the maximal margin, ie
    distance between the 2 margins = 2 / || W ||

4) The separating hyperplane satisfies the equation
    W dot X + b = 0   ---- [1]
    Any point above the separating hyperplane aka MMH satisfies
    w0 + w1 x1 + w2 x2 ... > 0
    and the one below the separating hyperplane (opposite class in binary classification) satisfies
    w0 + w1 x1 + w2 x2 + ... < 0

5) The complexity of the learned classifier is directly proportional to the number of support vectors rather than the
dimensionality of the data. Hence, SVMs tend to be less prone to over-fitting than other methods.

6) The number of support vectors found can be used to compute an upper bound on the expected error rate of the SVM
classifier, which is independent of the data dimensionality.

7) An SVM with a small number of support vectors can have a good generalization even when the data dimensionality
is high.

8) When classes are not linearly separable, find a non-linear transformation function to project X into a higher
dimensional latent space Z. For example, a 3D input vector X = (x1, x2, x3) can be mapped into a 6D space Z using the
mapping, Φ1(X) = x1,
         Φ2(X) = x2,
         Φ3(X) = x3,
         Φ4(X) = x1^2
         Φ5(X) = x1 x3
         Φ6(X) = x2 x3
         Now d(Z) = WZ + b is linearly separable.
                  = w1 x1 + w2 x2 + w3 x3 + w4 x1^2 + w5 x1 x2 + w6 x2 x3 + b

9) Using lagrangian formulation, equation [1] can be rewritten as teh decision boundary

d(XT) = Σ (i = 1 to l)  y_i α_i X_i X_T + b_0
    where l = number of support vectors,
    y_i = class label of support vector X_i
    X_T is a test tuple
    a_i and b_0 are numeric params determined by the SVM optz algo

10) Find W and the bias term such that 1 / || W || is maximized for all pairs of points. Hence, this is a quadratic
    time-complexity problem to solve and hence computationally slow (since we need to compute the dot product
    several times). It can reduce to a linear-time solution by some math tricks such as finding a kernel function
    K(X_i, X_j) and applying it to the original data in X rather than in higher-dim plane Z.

    Popular kernels are
    a) polynomial kernel of degree h
    b) gaussian radial basis function kernel
    c) sigmoid kernel

11) In practice, the kernel chosen does NOT make a big difference to accuracy.

12) SVM training always finds a global solution unlike NN, such as backpropagation where many local minima usually exist

13) very large datasets having millions of support vectors is still a computational challenge for production use cases

------------------------
Non-parametric models differ from parametric models. The model structure is not specified a priori
but is determined from data. Eg of parametric models are neural networks and support vector machines (SVM).
Examples of non-parametric models are k-nearest neighbors (KNN) and gaussian processes.
------------------------
an F-test in regression compares the fits of different linear models. Unlike t-tests that can assess only one regression coefficient at a time, the F-test can assess multiple coefficients simultaneously.

The hypotheses for the F-test of the overall significance are as follows:

Null hypothesis: The fit of the intercept-only model and your model are equal.
Alternative hypothesis: The fit of the intercept-only model is significantly reduced compared to your model.

If the P value for the F-test of overall significance test is less than your significance level, you can reject the null-hypothesis and conclude that your model provides a better fit than the intercept-only model.
------------------------
dummy variables - war_happens - binary 0 or 1 . to model qualitative outcomes
--------
Common questions asked for Machine Learning Engineering roles -
Conceptual understanding of
-----------------
1) over-fitting

Definition of regularization: Its a modification to the learning algorithm to reduce its
generalization error (ie, test error) but not its training error.

MOST IMPORTANT CONCEPT ABOUT REGULARIZATION - Its NOT a function of the data. Its a function of the weights W, and
    L2 norm biases towards choosing lower weight values W_i when there are multiple Ws giving the same loss.
SVM-loss = avg(data loss) over N examples + λ(regularized loss).
    The value of λ is usually set by cross-validation. Including the L2 regularized penalty leads to the appealing
    MAX-MARGIN property in SVMs. The intuition is that no particular input dimension can have an outsized-influence on
    the outcome and leads to better generalizations. Its common to regularize only the weights W and not the biases b.

Q) What is the advantage of combining L1 (ridge) and L2 (lasso aka least absolute shrinkage and
selection operator) regularization?
A) Lasso (aka L2, uses sum of squares of co-efficients) or ridge (aka L1, uses absolute value of coefficients) are forms of regularized linear regressions.
Linear regression in 1 variable can be written as
Y = aX + b + e (error)

The prediction error can be either due to bias or variance or both.
If multiple independent variables are present (may or may not be correlated),
it is represented as
Y = a1X1 + a2X2 + ...anXn + b + e

source: https://en.wikipedia.org/wiki/Tikhonov_regularization
Ordinary least squares seeks to minimize the sum of squared residuals, that can be compactly
represented as ||Y - AX||^2 where ||.|| is the euclidean norm. In order to give preference to a
particular solution with desirable properties, a regularization term can be included in this
minimization,
||Y - AX||^2 + ||Γx||^2
The matrix Γ is usually chosen as a multiple of the identity matrix Γ = αI, giving preference to 
solutions with smaller norms. This is known as L₂ regularization.
L₂ regularization is also used in other contexts such as classification with logistic regression, 
support vector machines and matrix factorization.

The coefficients in A are regularized to control the variance. 

Ridge regression solves the multi-co-linearity problem (correlated independent variables) through
 shrinkage parameter Γ. Γ controls the size of the coefficients as well as the amount of regularization.

Ridge regression can't zero coefficients.
 Here, YOU EITHER SELECT ALL THE COEFFICIENTS OR NONE OF THEM (very important property of L1 regularization) whereas

https://en.wikipedia.org/wiki/Lasso_(statistics)  
(refer to diagram where L1 is like a rotated square on the XY axes with vertexes on the X & Y axes, L2 looks like  a circle around origin)
LASSO DOES BOTH PARAMETER SHRINKAGE AND VARIABLE SELECTION AUTOMATICALLY (most important property of L2. Due to variable selection,
    property, it is more often used in Regression and Classification tasks as compared to L1) because
it zeros-out the co-efficients of collinear variables.
  a) Here it helps to select the variable(s) out of given n variables while performing lasso regression.
  b) If group of predictors are highly correlated, lasso picks only one of them and shrinks the others to zero

  c) The bayesian interpretation for Lasso is that the coefficients form a Laplace-distribution (looks like exponential decay), and hence
  tends to set more co-efficients to zero.
  d) Practical consideration - The ridge (L1) is a bit easier to implement (su, of absolute-values) and faster to compute
  (no squres that can arithmetic-overflow or square-roots), which may matter depending on the type of data you have.

  e) Thumb rule - Generally, when you have many small/medium sized effects you should go with ridge (L1).
  If you have only a few variables with a medium/large effect, go with lasso (L2). - Hastie, Tibshirani, Friedman

  f) In the disease prediction problem where you have to link the disease to 1 of 7k genes, with very limited number of training/test data samples,
  ridge (L1) is good at grouping similar (correlated genes) but does not completely eliminate trivial genes. Here, lasso is helpful.
  LASSO (L2) is good for eliminating trivial genes, but not good for grouped selection.

Short comings of Lasso - When the number of samples n < number of features (aka covariates) p, lasso can select only n covariates,
even when more are associated with the outcome. It also tends to select only 1 covariate from any set of highly correlated covariates. 
Additionally, even when n > p , ridge regularization tends to perform better. Elastic net extends lasso by adding an extra l^2 parameter,
||Y - AX||₁^2 + ||ΓA||₁^2 + Γ||A||₂ 


https://www.slideshare.net/ShangxuanZhang/ridge-regression-lasso-and-elastic-net
  a) Another type of regularization method is ElasticNet. It linearly combines the L1 and L2 penalties of the lasso and ridge methods.

  b) It introduces a quadratic term which makes the function strictly convex and hence,
  having a single minimum. Common applications - SVMs, portfolio optimization, metric learning.

  c) It is trained with L1 and L2 prior as regularizer.

  d) A practical advantage of trading-off between Lasso and Ridge is that, it allows Elastic-Net to inherit some of Ridge’s stability under rotation. (not sure what this means)

Pros and Cons of ElasticNet:
Pros - 
  a) Enforce sparsity
  b) No limitation on the number of selected variables
  c) Encourage grouping predictors in the presence of highly correlated predictors
Cons -
  a) Naive elastic net suffers from double shrinkage
---------------------------
2) cross-validation - For really large datasets, its usually not worth the trouble to do k-fold cross validation.
In this case, ML researchers just do train/test/validate split. Cross-validation becomes useful when the dataset is tiny (~100s of examples),
but you cant learn a complex model. 

----------------------------
3) convexity
4) classification
5) clustering
6) dimensionality reduction

Conceptual overview of the most common ML algorithms.

------------------------------------------------------------------------
MAR, 2019 - EDGAR VALESCO TALKS ON REINFORMCENT LEARNING LECTURES -

maximize expected cumulative reward.

stochastic tasks - every action can end up in different states depending on randomness and probabilities.

Agent takes an action in an env, and ends up in a new state.
You get (state, action, reward) pairs after each state and action.

Components of RL -
1) you need  POLICY, ie a mapping from states in the env and actions to take in that state.
It can be a look-up table, stochastic (ie probabilistic randomness) or search process.

2) REWARD - defines the goal of a reinforcement learning problem. In game, its to maximize the score.
Learning to run can be a more abstract reward tto define mathematically. Reward is not LOSS, since it
need not be differentiable.

Exploration vs exploitation trade-offs -
There's an approach  called episolon greedy which adds a bit of random noise.
Its not just simulated annealing . There's a cost to choosing a behaviour.
-------
MODEL-BASED VS MODEL-FREE METHODS -
A model of the environment. Used for planning eg in chess or alpha-go , use monte-carlo methods to
look ahead. Consider future situations before they're experienced.

Model based methods not very common in the real world.
Model-free methods are trial and error learners. Use value functions to learn. Eg - robot walking.
---
EVOLUTIONARY METHODS - genetic algorithms, simulated annealing, etc.
These never estimate value functions. Clone n envs, in each env, agent uses policy. You see which ones
collected the most reward. Then you use the best 10% cream of the crop, and then going to next permutations
to mimic evolution.

-------
Reward hypothesis for walking - walk fast + walk forward + walk smoothly + walk for as long as possible.

At time step t, the agent picks At to maximize (expected) Gt.
Discounted return Gt = R_t+1 + gamma R_t+2 +gamma^2 R_t+3
short-term reward vs long-term reward.

gamma is usually chosen as 0.9

---
Finite Markov Decision Processes (MDPs)

Probability of a state and reward depends only on previous state and action.
This is because you have to include all previous interactions in current interaction.


-----------------
**** ML conf 2016 notes ****

reinforcement learning (RL) by Harm van - Maluuba - 
Cons:
1) lots of data
2) sample distribution changes during learning
3) samples are not independent & identically distributed (iid)
--------------
some blogs & links reporting latest developments in AI/ML
http://www.offconvex.org/
http://approximatelycorrect.com/
https://jack-clark.net/import-ai/

------------------
what are the tradeoff one is willing to accept between model performance and computational
resources.

netflix - converting stills to polished assets
--------------------------------------------------------------------------
AirBnB -
Sizing the opportunity and scope
OKRs (objectives & key results) - have a business metric you want to move.
target metric = business outcome (not precision/recall of your model)

impact of pricing on booking + rebooking
price filter usage
variations by market

value is the best predictor to rebooking.
--------
model architecture - 
before: predict price from {location, recency, listing characteristics}
this mimicked host behaviour.

after: new metric: bookings
price suggestion based on probability of booked on given day - 
much more flexible
prices for each date
interesting UX opportunities
--------------------------

search: ranking model could optimize for click-through.
eg - beach vacations.

but not always relevant, eg - conference in houston.

now:
-----------------------
A9 - Amazon - Amazon Search 

1 model for books in france, another for cars in italy , etc
each model is small and specific.


positive labels - added to card, wishlist

negative- shown but not clicked
------------
mixing clicks and purchase targets -
for search query = iPhone
most clicked = apple iphone 7
most purchased = $8 lightning to USB cable

so now, ranking from a model trained on a mix of clicks and purchases.
-----------
fast feature evaluation - 
---------------------------

pinterest - 
multi-armed bandit model for merging ranks based on different criteria- from recommendations, from
past history, network of friends ,etc

sampling technique = thompson sampling based on user affinities.

affinity modeled as a beta distribution 
E(beta) = #actions / # views
---------------------------
personalization & scalable deep learning with MXNET - AWS Machine Learning


**** end ML conf 2016 notes *********

------------------------------------------------
L-BFGS is an optimization gradient descent algorithm for faster convergence, better than
traditional stochastic gradient descent (SGD), since tt takes curvature into account.

It models curvature as a (d x d) matrix of partial derivatives, and then takes the inverse of this
 hessian matrix.
Taking inverse is an O(d^3) operation and not scalable. Hence we use approximations instead.

exact formula:
w(t+1) = w(t) - Hinv * gt

approximation:
w(t+1) = w(t) - gamma_t * H_approx_inv * gt
where:
  a) gamma_t = step size computation, needs to satisfy some technical (wolfe) conditions. Adaptively
   determined from data.
  b) H_approx_inv = inverse hessian approximation (based on history of L-previous gradients and
  model deltas)

source: spark summit 2016 Yahoo talk 
https://www.youtube.com/watch?v=l_1S7W_l2cI&list=PL-x35fyliRwiz70bTSSK4HmOZ4JazCFUj&index=5
Yahoo has built their own parameter server on spark.
Subset of state vectors stored in different shards (scaling ML to billions of params using spask
MLLib @ Yahoo)
Different columns stored on different shards for efficiency of dot product, matrix multiplication
and other linear algebra computations.

Speedup tricks borrowed from Vowpal wabbit:
  1) intersperse communication and computation
  2) for quicker convergence, parallel line search for step size, curvature for initial hessian
  approximation
  3) Network bandwidth reduction using:
     compressed integer arrays
     only store indices for binary data
  4) matrix math on mini batch

------------------
Word Embeddings implementation on Spark MLLib at Yahoo:

word2vec (unsupervised learning and vector representation of words with embedded semantic meaning) 
skipGram with negative sampling - training set includes pairs of words and neighbors in corpus,
+ randomly selected words for each neighbor (negative sampling).

algorithm: determine w -> u(w), v(w) so that sigmoid(u(w) dot_product v(w')) is close to
(minimizes log loss) the probability that w' is a neighbor of w as opposed to a randomly selected
word.

SGD involces computing many dot products, eg  u(w).v(w')  and vector linear combos,
eg u(w) += a * v(w')
------------------
Spark Summit 2016: Distributed deep learning on spark at Baidu:
Typical deep learning applications & corresponding training data sizes at Baidu:
  image recognition - 100M
  optical character recognition (captcha for web auth) - 100M 
  speech - 10M
  click thru rate - 100B

Deep learning libraries comparison:
 
capability           | Caffe  | Tensor Flow          | Torch                | Paddle(Baidu)
--------------------------------------------------------------------------------------------
Distributed training | Yes    | Yes                  | No                   | Yes
communication cost   | medium | hi                   | N/A                  | medium to low
customizable code    | yes    | sharp learning curve | sharp learning curve | yes
sparse model support | no     | yes                  | yes                  | yes
area of focus        | vision | all                  | all                  | all
spark integration    | yes    | no                   | no                   | yes
-------
paper on XG Boost

XGBoost is a very popular gradient boosting decision tree library that often wins kaggle competitions.
gradient boosting is a technique to combine many weak learners or to iteratively improve a weak
learner into a strong learner.

eg - create residuals of F1(x)  = y , and then build a decision tree for the residuals, then take
the MSE (mean square error)
from that model h1(x) = y - F1(x) 
and then  F2(x) = F1(x) h1(x)
In general case, Fm(x) = Fm-1(x) + hm-1(x)

Although its an example shown on a decision tree in tutorial below, gradient boosting can be
applied to any tree or non-tree based model, to convert a weak learner into a strong learner.
http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/?utm_source=Mailing+list&utm_campaign=b94a4a50dc-Kaggle_Newsletter_03-01-2017&utm_medium=email&utm_term=0_f42f9df1e1-b94a4a50dc-398828801

Replacing MSE with MAE (mean absolute error) has 2 drawbacks. 
1) Computationally expensive. Since each split in the decision tree has to search for a median
2) It "ruins" our plug-n-play system. We'd only be able to plug-in weak learners that support the
objective function(s) (MAE) that we want to use.
On the other hand, many weak learners readily are available for MSE minimization.

Shrinkage is a concept of regularization of gradient boosted functions. 
Fm(x) = Fm-1(x) + v * gamma * hm(x) where 0 < v <= 1
If v < 0.1, it dramatically improves a model's generalization ability over gradient boosting without
 shrinkage (aka regularization)
The average of the differentiated function shows the general slope to walk towards. Step size in
that direction is determined by learning rate (0 < LR < 1)
In other words, each step is shrunken by some factor.

Pre-requisite: You should have a differentiable loss function for the algorithm to minimize.
Logistic function is typically used for binary classification and softmax function is often used for
 multi-class classification.

Gradient boosted decision tree - Misclassified training example weights are boosted and a new tree is formed.
  The final score is weighted sum of scores from all trees.

Decision tree regression, unlike decision tree classification, uses mean square error or similar metrics instead of
gini impurity or cross-entropy loss to determine splits.

************************************************************************************************************************
TIPS ON GRADIENT BOOSTING HYPER_PARAMETER TUNING (from http://explained.ai/gradient-boosting/index.html)

0) 2 main hyper parameters - the learning rate and the number of additive models in the gradient boosted ensemble.
1) better to keep a small learning rate, ~0.1
2) better to keep a large number of additive models (M) but not too large to over-fit.

--------
Random forest usually does not overfit. 
sci-kit learn currently only has one-hot encoding of categorical features which is a drawback that
will be fixed soon. one-hot encoding creates more sparse data that can be a disadvantage.
-------------
Notes from XG Boost paper - 
http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf

ensemble methods outperform XG Boost by only a very small amount. 
Scalability under all scenarios is the most important feature of XG Boost's success
-----------
GPUs do not result in a large improvement in training time when data is sparse. 
https://cloud.google.com/blog/big-data/2017/02/using-google-cloud-machine-learning-to-predict-clicks-at-scale
- uses largest click data ever released- by criteo labs, > 1 TB, 10B+ rows of click stream data

While an improvement in loss of 1.5% may seem small, it can make a huge difference in advertising
revenue or be the difference between first place and 15th place in machine-learning competitions.

Linear models are quite powerful, but we can achieve better results by using a deep neural network
(DNN). A neural network can also learn complex feature combinations automatically, removing the need
 to specify crosses (combinations of 2 or more columns. Combination of columns can be determined
 empirically).

parallelizing XGBoost implementation - 
1) cache-aware access - keep columnar storage on-disk compressed with parquet, which gives ~26%
compression. While CPU is utilized to uncompress in-memory, this can be used to hide disk-IO
latency.
2) If there are multiple disks and data is sharded across disks, you can have a shared in-memory
buffer, and different threads can fetch into the in-mem buffer, 1-thread from each disk.
Then another thread can read off the in-memory buffer and run the gradient descent or other loss
function minimizing step that is CPU-intensive.


Tree-based model is better at handling continuous features (like add_edit_dist and geo_dist and
name_edit_dist)
-----------
AuPR = area under the precision -recall curve where precision = y-axis and recall= x-axis. This uses the prediction of a model as the basis.

In this regard, its different from the area under the ROC (receiver operating characteristics) curve which is TPR (y-axis) vs FPR (x-axis). AUC uses the data as the basis rather than the model prediction.

TPR = same definition as precision while FPR = FP / (FP + TN). FPR is the lateral-inversion mirror image of recall.

AuPR is a good metric to use when there is heavily class-imbalanced data according to this paper.
http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf 

accuracy = (TP + TN) / (TP + TN + FP + FN)

--------------
https://medium.com/manifold-ai/torus-a-toolkit-for-docker-first-data-science-bddcb4c97b52
Tarus - Dockerize machine learning applications to have reproducible environments. Jupyter notebooks on browser
and your pycharm IDE can still run off a docker container that can be downloaded from docker hub cloud and shared by
your team. Cookiecutter scripts available to get started with the right project structure to get this up quickly.
Also comes with a gui if you're not familiar with docker CLI cmds. Its called "Kitematic" and is available from the
docker menu.

--------------

HINGE LOSS aka max-margin loss is the term used when they threshold to max 0 by using max(0, *). Squared hinge loss
    [max(0, *)]^2 is sometimes used that penalizes violated margins more strongly. Hinge loss is often used in
    multi-class SVM classifiers while cross-entropy loss is used in softmax classifiers.
-------------
Hyperparameter relative importances for binary classification - einstein modeling research for AuPR curves
~/Dropbox/Tech_extras/ML/hyperparameter-tuning-approaches-tradeOffs.png

Logistic Regression -  elasticNet (0.9) >> maxIter (0.1) >> reg (0.01) > tol (0.005)
Random Forest - maxDepth (0.7) >> numTrees (0.2) >> minInstances (0.05) > minInfoGain (0.03) > maxBin (0.02)
DecisionTree - minInstances (0.6) > maxDepth (0.4) >> maxBin (0.001) > minInfoGain (~0)
--------------
 
How is logistic regression affected by multi-collinearity?
Colinearity has two main effects on logistic regression:

The variances of the parameter estimates are large
The model is unstable - that is, small changes to the data can result in large changes to parameter estimates, even reversing their sign and making them significant in the other direction.
If your only goal is prediction (or categorization) then colinearity isn’t problematic. But you may get a very distorted idea of what the model is doing.

--------------
