TODO topics  -
1) markov chain monte carlo
------------------------
Non-parametric models differ from parametric models. The model structure is not specified a priori
but is determined from data. Eg of parametric models are neural networks and support vector machines (SVM).
Examples of non-parametric models are k-nearest neighbors (KNN) and gaussian processes.
------------------------
an F-test in regression compares the fits of different linear models. Unlike t-tests that can assess only one regression coefficient at a time, the F-test can assess multiple coefficients simultaneously.

The hypotheses for the F-test of the overall significance are as follows:

Null hypothesis: The fit of the intercept-only model and your model are equal.
Alternative hypothesis: The fit of the intercept-only model is significantly reduced compared to your model.

If the P value for the F-test of overall significance test is less than your significance level, you can reject the null-hypothesis and conclude that your model provides a better fit than the intercept-only model.
------------------------
dummy variables - war_happens - binary 0 or 1 . to model qualitative outcomes
--------
Common questions asked for Machine Learning Engineering roles -
Conceptual understanding of
-----------------
1) over-fitting

Definition of regularization: Its a modification to the learning algorithm to reduce its
generalization error (ie, test error) but not its training error.

MOST IMPORTANT CONCEPT ABOUT REGULARIZATION - Its NOT a function of the data. Its a function of the weights W, and
    L2 norm biases towards choosing lower weight values W_i when there are multiple Ws giving the same loss.
SVM-loss = avg(data loss) over N examples + λ(regularized loss).
    The value of λ is usually set by cross-validation. Including the L2 regularized penalty leads to the appealing
    MAX-MARGIN property in SVMs. The intuition is that no particular input dimension can have an outsized-influence on
    the outcome and leads to better generalizations. Its common to regularize only the weights W and not the biases b.

Q) What is the advantage of combining L1 (ridge) and L2 (lasso aka least absolute shrinkage and
selection operator) regularization?
A) Lasso (aka L2, uses sum of squares of co-efficients) or ridge (aka L1, uses absolute value of coefficients) are forms of regularized linear regressions.
Linear regression in 1 variable can be written as
Y = aX + b + e (error)

The prediction error can be either due to bias or variance or both.
If multiple independent variables are present (may or may not be correlated),
it is represented as
Y = a1X1 + a2X2 + ...anXn + b + e

source: https://en.wikipedia.org/wiki/Tikhonov_regularization
Ordinary least squares seeks to minimize the sum of squared residuals, that can be compactly
represented as ||Y - AX||^2 where ||.|| is the euclidean norm. In order to give preference to a
particular solution with desirable properties, a regularization term can be included in this
minimization,
||Y - AX||^2 + ||Γx||^2
The matrix Γ is usually chosen as a multiple of the identity matrix Γ = αI, giving preference to 
solutions with smaller norms. This is known as L₂ regularization.
L₂ regularization is also used in other contexts such as classification with logistic regression, 
support vector machines and matrix factorization.

The coefficients in A are regularized to control the variance. 

Ridge regression solves the multi-co-linearity problem (correlated independent variables) through
 shrinkage parameter Γ. Γ controls the size of the coefficients as well as the amount of regularization.

Ridge regression can't zero coefficients.
 Here, YOU EITHER SELECT ALL THE COEFFICIENTS OR NONE OF THEM (very important property of L1 regularization) whereas

https://en.wikipedia.org/wiki/Lasso_(statistics)  
(refer to diagram where L1 is like a rotated square on the XY axes with vertexes on the X & Y axes, L2 looks like  a circle around origin)
LASSO DOES BOTH PARAMETER SHRINKAGE AND VARIABLE SELECTION AUTOMATICALLY (most important property of L2. Due to variable selection,
    property, it is more often used in Regression and Classification tasks as compared to L1) because
it zeros-out the co-efficients of collinear variables.
  a) Here it helps to select the variable(s) out of given n variables while performing lasso regression.
  b) If group of predictors are highly correlated, lasso picks only one of them and shrinks the others to zero

  c) The bayesian interpretation for Lasso is that the coefficients form a Laplace-distribution (looks like exponential decay), and hence
  tends to set more co-efficients to zero.
  d) Practical consideration - The ridge (L1) is a bit easier to implement (su, of absolute-values) and faster to compute
  (no squres that can arithmetic-overflow or square-roots), which may matter depending on the type of data you have.

  e) Thumb rule - Generally, when you have many small/medium sized effects you should go with ridge (L1).
  If you have only a few variables with a medium/large effect, go with lasso (L2). - Hastie, Tibshirani, Friedman

  f) In the disease prediction problem where you have to link the disease to 1 of 7k genes, with very limited number of training/test data samples,
  ridge (L1) is good at grouping similar (correlated genes) but does not completely eliminate trivial genes. Here, lasso is helpful.
  LASSO (L2) is good for eliminating trivial genes, but not good for grouped selection.

Short comings of Lasso - When the number of samples n < number of features (aka covariates) p, lasso can select only n covariates,
even when more are associated with the outcome. It also tends to select only 1 covariate from any set of highly correlated covariates. 
Additionally, even when n > p , ridge regularization tends to perform better. Elastic net extends lasso by adding an extra l^2 parameter,
||Y - AX||₁^2 + ||ΓA||₁^2 + Γ||A||₂ 


https://www.slideshare.net/ShangxuanZhang/ridge-regression-lasso-and-elastic-net
  a) Another type of regularization method is ElasticNet. It linearly combines the L1 and L2 penalties of the lasso and ridge methods.

  b) It introduces a quadratic term which makes the function strictly convex and hence,
  having a single minimum. Common applications - SVMs, portfolio optimization, metric learning.

  c) It is trained with L1 and L2 prior as regularizer.

  d) A practical advantage of trading-off between Lasso and Ridge is that, it allows Elastic-Net to inherit some of Ridge’s stability under rotation. (not sure what this means)

Pros and Cons of ElasticNet:
Pros - 
  a) Enforce sparsity
  b) No limitation on the number of selected variables
  c) Encourage grouping predictors in the presence of highly correlated predictors
Cons -
  a) Naive elastic net suffers from double shrinkage
---------------------------
2) cross-validation - For really large datasets, its usually not worth the trouble to do k-fold cross validation.
In this case, ML researchers just do train/test/validate split. Cross-validation becomes useful when the dataset is tiny (~100s of examples),
but you cant learn a complex model. 

----------------------------
3) convexity
4) the kernel trick
5) classification
6) clustering
7) dimensionality reduction

Conceptual overview of the most common ML algorithms.

------------------------------------------------------------------------

**** ML conf 2016 notes ****

reinforcement learning (RL) by Harm van - Maluuba - 
Cons:
1) lots of data
2) sample distribution changes during learning
3) samples are not independent & identically distributed (iid)
-----------
deep reinforcement learning - 
method called DQN 


"Something of the previous state, however, survives every change. This is called in the language of
 cybernetics feedback, the advantages of learning from experience and of having developed reflexes."
 - Guy Davenport

--------------
some blogs & links reporting latest developments in AI/ML
http://www.offconvex.org/
http://approximatelycorrect.com/
https://jack-clark.net/import-ai/

------------------
what are the tradeoff one is willing to accept between model performance and computational
resources.

netflix - converting stills to polished assets
--------------------------------------------------------------------------
AirBnB -
Sizing the opportunity and scope
OKRs (objectives & key results) - have a business metric you want to move.
target metric = business outcome (not precision/recall of your model)

impact of pricing on booking + rebooking
price filter usage
variations by market

value is the best predictor to rebooking.
--------
model architecture - 
before: predict price from {location, recency, listing characteristics}
this mimicked host behaviour.

after: new metric: bookings
price suggestion based on probability of booked on given day - 
much more flexible
prices for each date
interesting UX opportunities
--------------------------

search: ranking model could optimize for click-through.
eg - beach vacations.

but not always relevant, eg - conference in houston.

now:
-----------------------
A9 - Amazon - Amazon Search 

1 model for books in france, another for cars in italy , etc
each model is small and specific.


positive labels - added to card, wishlist

negative- shown but not clicked
------------
mixing clicks and purchase targets -
for search query = iPhone
most clicked = apple iphone 7
most purchased = $8 lightning to USB cable

so now, ranking from a model trained on a mix of clicks and purchases.
-----------
fast feature evaluation - 
---------------------------

pinterest - 
multi-armed bandit model for merging ranks based on different criteria- from recommendations, from
past history, network of friends ,etc

sampling technique = thompson sampling based on user affinities.

affinity modeled as a beta distribution 
E(beta) = #actions / # views
---------------------------
personalization & scalable deep learning with MXNET - AWS Machine Learning


**** end ML conf 2016 notes *********

------------------------------------------------
L-BFGS is an optimization gradient descent algorithm for faster convergence, better than
traditional stochastic gradient descent (SGD), since tt takes curvature into account.

It models curvature as a (d x d) matrix of partial derivatives, and then takes the inverse of this
 hessian matrix.
Taking inverse is an O(d^3) operation and not scalable. Hence we use approximations instead.

exact formula:
w(t+1) = w(t) - Hinv * gt

approximation:
w(t+1) = w(t) - gamma_t * H_approx_inv * gt
where:
  a) gamma_t = step size computation, needs to satisfy some technical (wolfe) conditions. Adaptively
   determined from data.
  b) H_approx_inv = inverse hessian approximation (based on history of L-previous gradients and
  model deltas)

source: spark summit 2016 Yahoo talk 
https://www.youtube.com/watch?v=l_1S7W_l2cI&list=PL-x35fyliRwiz70bTSSK4HmOZ4JazCFUj&index=5
Yahoo has built their own parameter server on spark.
Subset of state vectors stored in different shards (scaling ML to billions of params using spask
MLLib @ Yahoo)
Different columns stored on different shards for efficiency of dot product, matrix multiplication
and other linear algebra computations.

Speedup tricks borrowed from Vowpal wabbit:
  1) intersperse communication and computation
  2) for quicker convergence, parallel line search for step size, curvature for initial hessian
  approximation
  3) Network bandwidth reduction using:
     compressed integer arrays
     only store indices for binary data
  4) matrix math on mini batch

------------------
Word Embeddings implementation on Spark MLLib at Yahoo:

word2vec (unsupervised learning and vector representation of words with embedded semantic meaning) 
skipGram with negative sampling - training set includes pairs of words and neighbors in corpus,
+ randomly selected words for each neighbor (negative sampling).

algorithm: determine w -> u(w), v(w) so that sigmoid(u(w) dot_product v(w')) is close to
(minimizes log loss) the probability that w' is a neighbor of w as opposed to a randomly selected
word.

SGD involces computing many dot products, eg  u(w).v(w')  and vector linear combos,
eg u(w) += a * v(w')
------------------
Spark Summit 2016: Distributed deep learning on spark at Baidu:
Typical deep learning applications & corresponding training data sizes at Baidu:
  image recognition - 100M
  optical character recognition (captcha for web auth) - 100M 
  speech - 10M
  click thru rate - 100B

Deep learning libraries comparison:
 
capability           | Caffe  | Tensor Flow          | Torch                | Paddle(Baidu)
--------------------------------------------------------------------------------------------
Distributed training | Yes    | Yes                  | No                   | Yes
communication cost   | medium | hi                   | N/A                  | medium to low
customizable code    | yes    | sharp learning curve | sharp learning curve | yes
sparse model support | no     | yes                  | yes                  | yes
area of focus        | vision | all                  | all                  | all
spark integration    | yes    | no                   | no                   | yes
-------
paper on XG Boost

XGBoost is a very popular gradient boosting library that often wins kaggle competitions.
gradient boosting is a technique to combine many weak learners or to iteratively improve a weak
learner into a strong learner.

eg - create residuals of F1(x)  = y , and then build a decision tree for the residuals, then take
the MSE (mean square error)
from that model h1(x) = y - F1(x) 
and then  F2(x) = F1(x) h1(x)
In general case, Fm(x) = Fm-1(x) + hm-1(x)

Although its an example shown on a decision tree in tutorial below, gradient boosting can be
applied to any tree or non-tree based model, to convert a weak learner into a strong learner.
http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/?utm_source=Mailing+list&utm_campaign=b94a4a50dc-Kaggle_Newsletter_03-01-2017&utm_medium=email&utm_term=0_f42f9df1e1-b94a4a50dc-398828801

Replacing MSE with MAE (mean absolute error) has 2 drawbacks. 
1) Computationally expensive. Since each split in the decision tree has to search for a median
2) It "ruins" our plug-n-play system. We'd only be able to plug-in weak learners that support the
objective function(s) (MAE) that we want to use.
On the other hand, many weak learners readily are available for MSE minimization.

Shrinkage is a concept of regularization of gradient boosted functions. 
Fm(x) = Fm-1(x) + v * gamma * hm(x) where 0 < v <= 1
If v < 0.1, it dramatically improves a model's generalization ability over gradient boosting without
 shrinkage (aka regularization)
The average of the differentiated function shows the general slope to walk towards. Step size in
that direction is determined by learning rate (0 < LR < 1)
In other words, each step is shrunken by some factor.

Pre-requisite: You should have a differentiable loss function for the algorithm to minimize.
Logistic function is typically used for binary classification and softmax function is often used for
 multi-class classification.

Gradient boosted decision tree - Misclassified training example weights are boosted and a new tree is formed.
  The final score is weighted sum of scores from all trees.
************************************************************************************************************************
TIPS ON GRADIENT BOOSTING HYPER_PARAMETER TUNING (from http://explained.ai/gradient-boosting/index.html)

0) 2 main hyper parameters - the learning rate and the number of additive models in the gradient boosted ensemble.
1) better to keep a small learning rate, ~0.1
2) better to keep a large number of additive models (M) but not too large to over-fit.

--------
Random forest usually does not overfit. 
sci-kit learn currently only has one-hot encoding of categorical features which is a drawback that
will be fixed soon. one-hot encoding creates more sparse data that can be a disadvantage.
-------------
Notes from XG Boost paper - 
http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf

ensemble methods outperform XG Boost by only a very small amount. 
Scalability under all scenarios is the most important feature of XG Boost's success
-----------
GPUs do not result in a large improvement in training time when data is sparse. 
https://cloud.google.com/blog/big-data/2017/02/using-google-cloud-machine-learning-to-predict-clicks-at-scale
- uses largest click data ever released- by criteo labs, > 1 TB, 10B+ rows of click stream data

While an improvement in loss of 1.5% may seem small, it can make a huge difference in advertising
revenue or be the difference between first place and 15th place in machine-learning competitions.

Linear models are quite powerful, but we can achieve better results by using a deep neural network
(DNN). A neural network can also learn complex feature combinations automatically, removing the need
 to specify crosses (combinations of 2 or more columns. Combination of columns can be determined
 empirically).

parallelizing XGBoost implementation - 
1) cache-aware access - keep columnar storage on-disk compressed with parquet, which gives ~26%
compression. While CPU is utilized to uncompress in-memory, this can be used to hide disk-IO
latency.
2) If there are multiple disks and data is sharded across disks, you can have a shared in-memory
buffer, and different threads can fetch into the in-mem buffer, 1-thread from each disk.
Then another thread can read off the in-memory buffer and run the gradient descent or other loss
function minimizing step that is CPU-intensive.


Tree-based model is better at handling continuous features (like add_edit_dist and geo_dist and
name_edit_dist)
-----------
AuPR = area under the precision -recall curve where precision = y-axis and recall= x-axis. This uses the prediction of a model as the basis.

In this regard, its different from the area under the ROC (receiver operating characteristics) curve which is TPR (y-axis) vs FPR (x-axis). AUC uses the data as the basis rather than the model prediction.

TPR = same definition as precision while FPR = FP / (FP + TN). FPR is the lateral-inversion mirror image of recall.

AuPR is a good metric to use when there is heavily class-imbalanced data according to this paper.
http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf 

accuracy = (TP + TN) / (TP + TN + FP + FN)

--------------
https://medium.com/manifold-ai/torus-a-toolkit-for-docker-first-data-science-bddcb4c97b52
Tarus - Dockerize machine learning applications to have reproducible environments. Jupyter notebooks on browser
and your pycharm IDE can still run off a docker container that can be downloaded from docker hub cloud and shared by
your team. Cookiecutter scripts available to get started with the right project structure to get this up quickly.
Also comes with a gui if you're not familiar with docker CLI cmds. Its called "Kitematic" and is available from the
docker menu.

--------------

Hinge loss aka max-margin loss is the term used when they threshold to max 0 by using max(0, *). Squared hinge loss
    [max(0, *)]^2 is sometimes used that penalizes violated margins more strongly. Hinge loss is often used in
    multi-class SVM classifiers while cross-entropy loss is used in softmax classifiers.
-------------
Hyperparameter relative importances for binary classification - einstein modeling research for AuPR curves

Logistic Regression -  elasticNet (0.9) >> maxIter (0.1) >> reg (0.01) > tol (0.005)
Random Forest - maxDepth (0.7) >> numTrees (0.2) >> minInstances (0.05) > minInfoGain (0.03) > maxBin (0.02)
DecisionTree - minInstances (0.6) > maxDepth (0.4) >> maxBin (0.001) > minInfoGain (~0)
--------------
 
